Ch. 5 Synchronizing Access to Shared Objects
(shared memory) (shared state) (state data)

Only independent threads that operate on completely separate
subsets of state, i.e. memory, then we can reason about each
thread separately.

This is as if each thread is a single sequential program.
We don't need to synchronize memory access.

However, most multithreaded (MT) programs consist of
cooperating threads that read and write to shared state
(memory).

When we have cooperating threads accessing shared state,
programming becomes much more difficult.

The sequential model for reasoning about (cooperating) threads
breaks down when:
1. Program execution depends on the interleavings of the
   thread's access to shared state.
   Ex. Thread T1 writes value 1 to x, 
       thread T2 writes value 2 to x. Final value depends on
	   which threads' write operation finishes last.
2. Program execution can be non-deterministic (randomness)
   (different runs of the same program can produce different
    results)
   a. Scheduler may make different scheduling decisions
   b. Processor may run at (slightly) different frequencies.
   c. Another concurrent process may affect the cache hit 
      rate.
   d. Running program with a debugger
   e. Recompiling with the -g instead -O options.

   Jim Gray: 
   Heisenbugs: bugs that disappear of change its behavior
   when you try to examine them.
   MT programs is a common source of Heisenbugs.
   
3. Compilers and architectures reorder instructions

Conclusions: We need structured synchronization to enable
  sharing between threads.
  
  Give up some freedom, and consistently follow some rules,
  we can reason about MT programs more easily.

1. Structure MT program's state as shared objects;
   define and limit how state can be accessed.
2. Shared objects include synchronization primitives
   (locks and condition variables) to coordinate access
   to their state by different threads.
3. Set of best practices for writing code that implements
   each shared object.

5.1 Challenges

Definition: Race Condition
Output of a concurrent program depends on the order of
operations (instructions) between threads.

The threads run a race between their operations, and the
results of the program execution depends on who wins the
race.

Ex.1: 
Thread 1			Thread 2
x=1					x=2

Q: What are the possible final values of x?
A: x=1 or x=2, depending on which thread wins or losses
   the "race" to set x.
   
Ex.2: Initially, y=12 (x need not be set)
Thread 1			Thread 2
x=y+1				y=y*2

Q: What are the possible final values of x?
A: We can get x=13 if T1 executes first,
   We can get x=25 if T2 executes first.

Thread 1			Thread 2
LOAD $y, D0			LOAD $y, D1
ADD #1, D0			MULT #2, D1
STORE D0, $x		STORE D1, $y

Ex.3: Initially, x=0
Thread 1			Thread 2
x=x+1				x=x+2

Q: What are the possible final values of x?
A: We can obviously get x==3. But we can also
   get x==2 and x==1. To see why, we need to
   mentally disassemble to pseudo-assembly code.

Thread 1			Thread 2	
LOAD $x, D0	        LOAD $x, D0	
ADD #1, D0	        ADD #2, D0	
STORE D0, $x        STORE D0, $x

---------------------------------
Interleaving 1:
---------------------------------
Thread 1			Thread 2	
LOAD $x, D0	        
ADD #1, D0	        
STORE D0, $x        
					LOAD $x, D0	
                    ADD #2, D0	
                    STORE D0, $x
---------------------------------

---------------------------------
Interleaving 2: Give x==2
---------------------------------
Thread 1			Thread 2	
LOAD $x, D0	        
					LOAD $x, D0
ADD #1, D0	        
					ADD #2, D0
STORE D0, $x        
                    STORE D0, $x
---------------------------------

---------------------------------
Interleaving 3: Give x==1
---------------------------------
Thread 1			Thread 2	
LOAD $x, D0	        
					LOAD $x, D0
ADD #1, D0	        
					ADD #2, D0
					STORE D0, $x
STORE D0, $x        
---------------------------------

Def. Atomic operations
Indivisible operations that cannot be interleaved with other
operations.

- On modern processors load/store of a 32-bit word to/from
  memory is an atomic operation.
- Storing a 64-bit word (floating point register) to memory
  is typically not atomic.


Too much milk

Intricacies of locking by memory only!

Model two room mates as threads.
Number of bottles of milk in the frigde.

Time	Person A						Person B
12:30	Look in fridge. Out of milk.
12:35	Leave for store
12:40	Arrive at store					Look in fridge. Out of milk
12:45	Buy milk						Leave for store
12:50	Arrive home, put milk away		Arrive at store
12:55									Buy milk
13:00									Arrive home, put milk away (ops!)

Def. Safety: The program never enters a bad state.

Def. Liveness: The program eventually enters a good state.
(sooner or later, program does something useful)

For the Too much milk example:
1. Safety: At most one person buys milk
2. Liveness: If milk is needed, eventually somebody buys 
milk.
   (can take a while)
   
Attempt 1: Leave a note on fridge

Each thread (room mate) executes:

if milk == 0 {			// if no milk
	if !note {			// if no note
		note = true		// leave a note
		milk++			// buy milk
		note = false	// remove note
	}
}

Problems:
- First thread execute until check if no milk, and the
  context switch.
- The other thread can run through all code 
  (including buying milk)
- First thread rescheduled and see that note is false,
  leave a note, buy milk etc.
- This code may work fine during testing. But the above
  problem shows that there is a Heisenbug!
  Very hard to reproduce.

Attempt 2: Use distinct notes per roommate (two variables)

Person A						Person B
--------                        --------
noteA=true                      noteB=true
if !noteB {              A1     if !noteA {            B1
	if milk == 0 {       A2      	if milk == 0 {     B2
		milk++           A3      		milk++         B3
	}                           	}                  B4
}                               }                      B5
noteA=false                     noteB=false

By using two variables (noteA,noteB) have we solved the 
problem?

We can check the safety property: At most one person buys
milk. Can also be stated as an invariant: 
  (milk==0 OR milk==1)

Def. Stable Property (variable)
A property that once it becomes true, remains true forever.

Safety Proof: Proof by contradiction:
Our assumption is: Algorithm is not safe, and both A and B
buy milk. Consider the state of the two variables
(noteB,milk) when thread A is at A1 at the moment when load
of noteB from memory to A's register occurs. There are
three cases to consider:

1. Case (true,*): Impossible because this state contradicts
the assumption that thread A buys milk and reaches A3.
(Thread A won't be buying milk when noteB=true.)

2. Case (false,milk>0): Impossible because property milk>0
is a stable property. Thus if milk>0 is true when A is at
A1, A's test at line A2 will fail, and A will not buy milk,
contradicting the assumption.

3. Case (false,milk=0): We know that thread B cannot be
executing lines B1-B5. We also know that either noteA=true
or milk>0 will be true from this time forward. (noteA OR
milk>0 is also a stable property). But this means that B
cannot buy milk in the future (either the test at B1 or B2
must fail), which contradicts our assumption that both A
and B buy milk. Q.E.D.

Liveness. Attempt 2 does not ensure liveness. It is
possible for both threads to set their respective notes,
for each thread to check the other thread's note, and for
both threads to decide not to buy milk. (Deadlock)

For solution 3: make sure that at least one of the threads
determine whether the other thread has bought milk or not
before deciding whether or not to buy milk.

Attempt 3: Use spinning to wait for B to remote his note.

Person A						Person B
--------                        --------
noteA=true                      noteB=true
while !noteB {                  if !noteA {       
	;                            	if milk == 0 {
}                     			 		milk++         
if milk == 0 {        M     	    }                  
	milk++                      }                      
}                               noteB=false
noteA=false                     

Safety can be shown with a similar argument to solution 2.

Liveness. Observe that Path B has no loops, so eventually
thread B must finish and set noteB=false, which remains
false forever (stable property). Therefore, eventually
thread A must read line M and decide whether to buy milk.
 - if milk==1, then milk is bought and we are live.
 - if milk==0, then thread A will buy milk, and we are live.

Discussion:
Solution to "Too much milk" can be made both live and safe,
using only atomic load and store operations on shared 
memory. However, the solution is:
- Complex: requires careful reasoning to convince oneself 
  that it works
- Asymmetric: code for thread A and B are slightly 
  different. 
  With more threads even more difficult.
- Inefficent: thread A is busy-waiting, consuming CPU.

Better solution is to use Lock primitive.

Kitchen::buyMilkIfNeed() {
	lock.acquire();
	if (milk==0) {
		milk++;
	}
	lock.release();
}

In Java:
Lock lock = new ReentrantLock();

void buyMilkIfNeeded() {
	lock.lock();
	try {
		if (milk==0) {
			milk++;
		}
	} finally {
		lock.unlock();
	}
}

In Go, we might use the defer keyword:

	var lock *sync.Mutex
	lock = &sync.Mutex{}

func buyMilkIfNeeded() {
	lock.acquire()
	defer lock.release()
	if milk == 0 {
		milk++
	}
}

Ch. 5.2 Shared Objects and Synchronization Variables

Def. Shared Object:
Object that can safely be accessed by multiple threads.

(OO principles of encapsulation to hide synchronization)

All shared state in a program should be encapsulated in one
or more shared objects.
- variables on the heap
- static and global variables

Shared objects extends OO programming
- Objects hide their implementation details behind clean
  interfaces
- Shared objects hide the details of synchronizing the
  actions of multiple threads. Threads using shared objects
  only needs to understand the interface; they don't need to
  know how synchronization is handled internally.

Synchronization Variables are member variables of Shared 
Object
- They are also stored in memory, but are carefully 
  designed for synchronization (locks)
- they are used to coordinate access to shared variables
such as ints, strings, arrays,....
- they simplify implementing shared objects
- implementing with sync. variables are very similar to 
  implementing data structures for single-threaded programs.


Ch 5.3 Lock: Mutual Exclusion

Def. Lock
A lock is a synchronization variable that provide mutual
exclusion.
- when one thread holds the lock, no other thread can 
  hold the lock (they are excluded).

A lock is associated with a subset of shared state and
requires a thread to hold the lock when accessing that
state.

Def. Mutual Exclusion
Only one thread does a particular thing at a time.
(particular thing => the critical section)

A thread can perform any operation on shared data protected
by the lock, and those operations will appear to be atomic
with respect to other threads.

Def. Critical Section
A chunk of code that accesses shared state.

A critical section should only be executed by one thread at 
at time.

How to use a lock:
- lock before entering critical section
  (before accessing shared data)
- unlock when leaving critical section
  (after we are done accessing shared data)
- wait if locked
  (all synchronization involves waiting!)

	lock.lock()
	// Critical Section starts here
	if milk == 0 {
		milk++
	}
	// Critical Section end here
	lock.unlock()

Lock API and Properties:
- Lock states: Busy and Free
- Initially Free.
- Lock::acquire() waits until the lock is Free
	- Then atomically makes the lock Busy
	- Atomic CPU instruction, e.g test-and-set.
- Lock::release() makes the lock Free
	- If there are pending acquire() operations
		- One of them will proceed

Formal Properties of Locks
1. Mutual Exclusion (Safety property). At most one thread 
   holds the lock.

2. Progress (Liveness property). If no thread holds the
   lock and any thread attempts to acquire the lock, then
   eventually some thread succeeds to acquire the lock.

3. Bounded waiting (Liveness and Fairness property).
   If thread T attempts to acquire the lock, then there
   exists a bound on the number of times other threads 
   successfully acquires the lock before T does.

PS: Locks do not order acquire() calls among threads (no 
FIFO ordering).


---------------------- AFTER BREAK ------------------------

Ch. 5.4 Condition Variables: Waiting for a change

To wait for another thread for some change:
Poll - repeatly check the state of interest

F.ex. TSQueue could wrap tryRemove() in a polling loop
to provide a remove() method that always returns an item.

func (q *TSQueue) remove() (item int) {
  success := false
  for !success {
  	success = q.tryRemove(&item)
	if !success {
		sleep(100ms)
	}
  }
  return
}

This approach is inefficient!
- continuously loops, consuming CPU cycles
- worse: it may delay scheduling of other threads

Fix: We could add a sleep inside the loop to avoid wasting 
CPU cycles?
- Waiting 100ms after each failed attempt to remove an item

Two problems:
- Even though we reduce the the inefficiency of polling
  it does not eliminate it.
  Suspending and resuming a thread imposes overheads
- Periodic polling adds latency (e.g. 100ms delay waiting
  for the next key press, or if it is a packet processing
  loop, 100ms may be too long)

Def. Condition Variable (CV)
A condition variable (betingelsevariable) is a
synchronization object that enables a thread to efficiently
wait for a change to shared state that is protected by a
lock. Essentially: wait for another thread to take some
action.

In the TSQueue example:
Instead of return an error when trying to remove an item
from an empty queue, we may instead wait until the queue is
non-empty and always return an item. (Making it BLOCKING)

API of CV:
- wait():
  atomically release lock and relinquish CPU until signaled
- signal():
  wake up a waiter, if any
- broadcast():
  wake up all waiters, if any

To use CV: Always hold lock when calling wait(), signal() 
and broadcast().
- CV is used to synchronize on shared state

Wait call must be in a loop:

lock.acquire()
while !conditionOnSharedState() {
	condition.wait(&lock)
}
// read and write to the shared state (critical section)
lock.release()

1. CV is memoryless
- if signaled when no thread is waiting, no op
- if wait before signal, waiter wakes up

2. Wait() atomically releases lock
- put thread on CV's waiting queue
- when wait() returns, must re-check condition
  (lock was released by wait(), and another thread may
   have changed the condition.)

3. When a thread is woken up from wait, it may not run
   immediately
- Signal/broadcast put thread on ready list
- When lock is released, anyone can acquire the lock

Method that signals:

methodThatSignals() {
  lock.acquire()
  // update the state that could allow other threads
  // to make progress
  cv.signal()
  lock.release()
}

Example: Blocking Bounded Queue
See Code example bbq.go

----

Def. Semaphore
A semaphore is a variable that may be used for controlling
access to a common resource by multiple threads
- Counting semaphore: resource count

--- Skip Ch. 5.5 (read on your own if interested)

Ch. 5.6 Design and implementing shared objects

Designing the class for a shared object similar to designing a single-threaded object.

1. Decompose the problem into objects
2. For each object
   a. Define a clean interface
   b. State: Identify the right internal state and 
      invariants (to support that interface)
   c. Algorithms: Implement methods with appropriate 
      algorithms that manipulate the state

Additionally for multi-threaded programs:

- Add a lock
  - only one lock per shared object

- Add code to acquire and release the lock
  - Reminder: Acquiring an uncontended lock is inexpensive
  - So don't avoid using locks for the wrong reasons

- Identify and add condition variables
  - Systematic approach:
    - For each method ask: "When can this method wait?"
	- Then map each situation (in which the method can wait)
	  to a condition variable
	  ex. itemRemoved, itemAdded

- Add loops to wait() using the condition variable
  - Can be difficult to define details of the condition test
    - Write private method first, e.g. workAvailable()
	- Then define private method later

while (!workAvailable()) {
	cond.wait(&lock)
}
// here the condition is true and we have the lock

- Add signal() and broadcast() calls
  - When can a method let other waiting threads proceed?
    - Ask: "Can a call to this method allow 
	        another thread to proceed?"
	- Add signal() or broadcast() if yes.
  - Use signal() when
    - when at most one waiting thread can make progress
	- any thread waiting on the CV can make progress
  - Use broadcast() when
    - multiple waiting threads may be able to make progress,
	- or different threads are using the same CV to wait for
	  different predicates (test conditions), so some of the
	  waiting threads can make progress but others cannot.
  - Always safe to use broadcast()
    - Consumes additional resources
	- But will not introduce bugs

----
Implementing best practices:
Writing simple, safe code with shared objects

Consistent structure
Meta-rule: Always follow a consistent structure 
(used established design patterns for shared objects)

(a) Frees you to focus on the core problem because details 
    of the standard approach become a habit
(b) Makes it easier for other people to review, maintain, or
    debug your code.
	
Five simple rules (best practices):
1. Always synchronize with locks and condition variables
(learn about semaphores to understand legacy code, but only
write new code with locks and CVs.)

2. Always acquire the lock at the beginning of a method
   and release it right before the return.
   (Keep consistent structure. If you find that a method 
    modifies shared state in a smaller part of the method, 
	then refactor it into a separate method that follows 
	Rule #2.)

3. Always hold the lock when operating on a condition 
   variable.
   
4. Always wait in a while/for loop
   while predicateOnStateVariables() {
     cond.Wait(&lock)
   }

5. (Almost) never sleep().
   Never use sleep() to have one thread wait for another 
   thread to do something. Correct way to wait for a 
   condition to become true is to wait() on a condition 
   variable.

Java pitfalls:
1. Avoid defining a synchronized block in the middle of a
   method.
2. Keep shared state classes (shared objects) separate from
   thread classes. State that can be accessed by multiple
   threads, locks, and condition variables should never
   appear in any Java class that extends Thread or 
   implements Runnable. Define SharedObject class as 
   separate from the Runnable thread class.

Example: Readers/Writers
- Database with records that can be read and written
- Want to maximize performace:
  - Multiple threads can read a record simultaneously
    (this is safe!)
  - For correctness: If one thread is writing a record, no
    other thread can read or write that record at the same
	time.

Need to generalize our mutex lock into readers-writers lock 
(RWLock)

To read a record, a thread should do:

	rwLock.startRead();
	// read database entry
	rwLock.doneRead();

To write a record, a thread should do:

	rwLock.startWrite();
	// write database entry
	rwLock.doneWrite();









































